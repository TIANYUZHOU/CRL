{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "class Bert_Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config, out_token=False):\n",
    "        super(Bert_Encoder, self).__init__()\n",
    "\n",
    "        # load model\n",
    "        self.encoder = BertModel.from_pretrained(config.bert_path).cuda()\n",
    "        self.bert_config = BertConfig.from_pretrained(config.bert_path)\n",
    "\n",
    "        # the dimension for the final outputs\n",
    "        self.output_size = config.encoder_output_size\n",
    "        self.out_dim = self.output_size\n",
    "\n",
    "        # find which encoding is used\n",
    "        if config.pattern in ['standard', 'entity_marker']:\n",
    "            self.pattern = config.pattern\n",
    "        else:\n",
    "            raise Exception('Wrong encoding.')\n",
    "\n",
    "        if self.pattern == 'entity_marker':\n",
    "            self.encoder.resize_token_embeddings(config.vocab_size + config.marker_size)\n",
    "            self.linear_transform = nn.Linear(self.bert_config.hidden_size*2, self.output_size, bias=True)\n",
    "        else:\n",
    "            self.linear_transform = nn.Linear(self.bert_config.hidden_size, self.output_size, bias=True)\n",
    "\n",
    "        self.layer_normalization = nn.LayerNorm([self.output_size])\n",
    "\n",
    "\n",
    "    def get_output_size(self):\n",
    "        return self.output_size\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # generate representation under a certain encoding strategy\n",
    "        if self.pattern == 'standard':\n",
    "            # in the standard mode, the representation is generated according to\n",
    "            #  the representation of[CLS] mark.\n",
    "            output = self.encoder(inputs)[1]\n",
    "        else:\n",
    "            # in the entity_marker mode, the representation is generated from the representations of\n",
    "            #  marks [E11] and [E21] of the head and tail entities.\n",
    "            e11 = []\n",
    "            e21 = []\n",
    "            # for each sample in the batch, acquire the positions of its [E11] and [E21]\n",
    "            for i in range(inputs.size()[0]):\n",
    "                tokens = inputs[i].cpu().numpy()\n",
    "                e11.append(np.argwhere(tokens == 30522)[0][0])\n",
    "                e21.append(np.argwhere(tokens == 30524)[0][0])\n",
    "\n",
    "            # input the sample to BERT\n",
    "            tokens_output = self.encoder(inputs)[0] # [B,N] --> [B,N,H]\n",
    "            output = []\n",
    "            # for each sample in the batch, acquire its representations for [E11] and [E21]\n",
    "            for i in range(len(e11)):\n",
    "                if inputs.device.type in ['cuda']:\n",
    "                    instance_output = torch.index_select(tokens_output, 0, torch.tensor(i).cuda())\n",
    "                    instance_output = torch.index_select(instance_output, 1, torch.tensor([e11[i], e21[i]]).cuda())\n",
    "                else:\n",
    "                    instance_output = torch.index_select(tokens_output, 0, torch.tensor(i))\n",
    "                    instance_output = torch.index_select(instance_output, 1, torch.tensor([e11[i], e21[i]]))\n",
    "                output.append(instance_output) # [B,N] --> [B,2,H]\n",
    "            # for each sample in the batch, concatenate the representations of [E11] and [E21], and reshape\n",
    "            output = torch.cat(output, dim=0)\n",
    "            output = output.view(output.size()[0], -1) # [B,N] --> [B,H*2]\n",
    "            \n",
    "            output = self.linear_transform(output)\n",
    "\n",
    "\n",
    "\n",
    "        return output\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Bert_Encoder(args)\n",
    "        self.output_size = self.encoder.out_dim\n",
    "        dim_in = self.output_size\n",
    "        self.head = nn.Sequential(\n",
    "                nn.Linear(dim_in, dim_in),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(dim_in, args.feat_dim)\n",
    "            )\n",
    "    def bert_forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        xx = self.head(out)\n",
    "        xx = F.normalize(xx, p=2, dim=1)\n",
    "        return out, xx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.bert_path = \"datasets/bert-base-uncased\"\n",
    "        self.encoder_output_size = 768\n",
    "        self.pattern = \"entity_marker\"\n",
    "        self.vocab_size = 30522\n",
    "        self.marker_size = 4\n",
    "        self.feat_dim = 64\n",
    "        self.device = 'cuda'\n",
    "        self.num_workers = 0\n",
    "        self.num_protos = 20\n",
    "        self.seed = 2023\n",
    "        self.max_length = 256\n",
    "        self.bert_path = \"datasets/bert-base-uncased\"\n",
    "        self.dataname = \"FewRel\"\n",
    "        self.task_name = \"FewRel\"\n",
    "        self.data_path = \"sample_data/\"\n",
    "        self.num_of_relation = 80\n",
    "        \n",
    "        \n",
    "args = Config()\n",
    "\n",
    "encoder = Encoder(args=args).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from dataloaders.my_sampler import data_sampler\n",
    "from dataloaders.data_loader import get_data_loader\n",
    "\n",
    "sampler = data_sampler(args=args, seed=args.seed)\n",
    "\n",
    "dataset_id2sample, dataset_rel2sample = next(sampler)\n",
    "\n",
    "current_relations = sampler.id2rel\n",
    "\n",
    "def select_data(self, args, encoder, sample_set):\n",
    "    data_loader = get_data_loader(args, sample_set, shuffle=False, drop_last=False, batch_size=1)\n",
    "    features = []\n",
    "    encoder.eval()\n",
    "    for step, batch_data in enumerate(data_loader):\n",
    "        labels, tokens, ind = batch_data\n",
    "        tokens=torch.stack([x.to(args.device) for x in tokens],dim=0)\n",
    "        with torch.no_grad():\n",
    "            feature, rp = encoder.bert_forward(tokens)\n",
    "        features.append(feature.detach().cpu())\n",
    "\n",
    "    features = np.concatenate(features)\n",
    "    num_clusters = min(20, len(sample_set))\n",
    "    distances = KMeans(n_clusters=num_clusters, random_state=0).fit_transform(features)\n",
    "\n",
    "    mem_set = []\n",
    "    current_feat = []\n",
    "    for k in range(num_clusters):\n",
    "        sel_index = np.argmin(distances[:, k])\n",
    "        instance = sample_set[sel_index]\n",
    "        mem_set.append(instance)\n",
    "        current_feat.append(features[sel_index])\n",
    "    \n",
    "    current_feat = np.stack(current_feat, axis=0)\n",
    "    current_feat = torch.from_numpy(current_feat)\n",
    "    return mem_set, current_feat, current_feat.mean(0)\n",
    "\n",
    "for relation in current_relations:\n",
    "    memorized_samples[relation], feat, temp_proto = self.select_data(args, encoder, dataset_rel2sample[relation])\n",
    "    feat_mem.append(feat)\n",
    "    proto_mem.append(temp_proto)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
